{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcNECtS4TzcU"
      },
      "source": [
        "# <font color='Blue'> **The required packages**</font>\n",
        "\n",
        "\n",
        "\n",
        "> **Torch package**\n",
        "*   The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors.\n",
        "\n",
        "> **Torch cuda**\n",
        "* It is used to set up and run CUDA operations. It keeps track of the currently selected GPU, and all CUDA tensors you allocate will by default be created on that device.\n",
        "\n",
        ">**Torch utils**\n",
        "*  TorchUtils is a Python package providing helpful utility APIs for the PyTorch projects.\n",
        "\n",
        "> **Torch nn**\n",
        "*  This contains different classess that help to build neural network models.\n",
        "\n",
        "> **Torch funtional**\n",
        "*  The functional API of PyTorch is a powerful tool that enables you to write\n",
        " high-performance neural network models\n",
        "\n",
        "\n",
        "> **Torch optim**\n",
        "*   torch.optim is a package implementing various optimization algorithms.\n",
        "\n",
        "> **Torchvision**\n",
        "\n",
        "*   Torchvision provides additional functionalities to manipulate and process images with standard image processing algorithms. It has the computer vision models and datasets\n",
        "\n",
        "\n",
        "1.   **datasets:**\n",
        "\n",
        "        It has common datasets like MNIST(Modified National Institute of Standards and Technology), CIFAR10, ImageNet etc.\n",
        "\n",
        "2.  **transforms**\n",
        "\n",
        "       Torchvision supports common computer vision transformations in the torchvision.transforms and torchvision.transforms.v2 modules. Transforms can be used to transform or augment data for training or inference of different tasks (image classification, detection, segmentation, video classification).\n",
        "\n",
        "\n",
        "> **Torch Summary**\n",
        "\n",
        "*  This can be used to print out the trainable and non-trainable parameters in a Keras-like manner for PyTorch models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJptKBxALl-u",
        "outputId": "6798087e-4647-462e-932b-d1f32c0f237f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "# !pip install torchsummary\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IfKaQkMVmOQ"
      },
      "source": [
        "**Torch Cuda**\n",
        "\n",
        "`It keeps track of the currently selected GPU, and all CUDA tensors you allocate will by default be created on that device.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install torch\n",
        "#!pip install numpy\n",
        "#!pip install torchsummary\n",
        "#!pip install torchvision\n",
        "#\n",
        "# !pip3 install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Owi1LBNY8L",
        "outputId": "eb1160fa-ac01-47f9-f34d-3ac8a7f84385"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaLKt1hVlnF2"
      },
      "source": [
        "#<font color='Blue'>  **What Does a PyTorch DataLoader Do?** </font>\n",
        "\n",
        "The PyTorch DataLoader class is a tool which help to prepare, manage, and serve the data to the deep learning networks. Because many of the pre-processing steps we will need to do before beginning training a model, finding ways to standardize these processes is critical for the readability and maintainability of your code.\n",
        "\n",
        "The PyTorch DataLoader used below are :\n",
        "\n",
        "**Define a dataset to work with:** identifying where the data is coming from and how it should be accessed.\n",
        "The datasets used here is the MNIST dataset\n",
        "\n",
        "**Batch the data:**\n",
        " To define how many training or testing samples to use in a single iteration. Because data are often split across training and testing sets of large sizes, being able to work with batches of data can allow us  training and testing processes to be more manageable.\n",
        "\n",
        "**Shuffle the data:** PyTorch can handle shuffling data for us as it loads data into batches. This can increase representativeness in the dataset and prevent accidental skewness.\n",
        "\n",
        "**Transforms:**\n",
        "\n",
        "The transform() method allows you to execute a function for each value of the DataFrame.\n",
        "\n",
        "TensorFlow Transform is a library for preprocessing input data for TensorFlow, including creating features that require a full pass over the training dataset. For example, using TensorFlow Transform you could: Normalize an input value by using the mean and standard deviation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPwj9wpmmaqe"
      },
      "source": [
        "# **Transforms**\n",
        "Data transformation is also known as data preparation or data preprocessing. There are lots of different names for the same thing. It makes sure that your data is clean and ready to be used by your machine learning algorithm. Without data transformation, your AI won't be able to make accurate predictions.\n",
        "\n",
        "**To Tensor**\n",
        "\n",
        "This is a very commonly used conversion transform. In PyTorch, we mostly work with data in the form of tensors. If the input data is in the form of a NumPy array or PIL image, we can convert it into a tensor format using ToTensor.\n",
        "\n",
        "Tensor image are expected to be of shape (C, H, W), where C is the number of channels, and H and W refer to height and width. Most transforms support batched tensor input. A batch of Tensor images is a tensor of shape (N, C, H, W), where N is a number of images in the batch. The v2 transforms generally accept an arbitrary number of leading dimensions (..., C, H, W) and can handle batched images or batched videos.\n",
        "\n",
        "**Normalization**\n",
        "\n",
        "The goal of normalization is to transform features to be on a similar scale. This improves the performance and training stability of the model.\n",
        "The original pixel values range from 0 to 255. So, we divided them by 255 to get them into the range of 0.0 to 1.0. That's normalizing! The benefit of normalizing the input data is that it avoids large gradient values that could make the training process difficult."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQZaZRGcNLtr",
        "outputId": "1c38a09b-829d-40bd-a193-60781f5fcb54"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "#ToTensor transforms the PIL Image to a torch.Tensor and Normalize subtracts the mean and divides by the standard deviation you provide.\n",
        "\n",
        "train_dataset = datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ]))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset\n",
        "    ,\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "test_dataset = datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ]))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset\n",
        "    ,\n",
        "    batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpjZtwp1mY3d"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5vLNt8Y0DdE",
        "outputId": "6c5ecd79-49e5-41ee-818d-37b671377401"
      },
      "outputs": [],
      "source": [
        "# nn.Conv2d(1,32,3,padding=1)\n",
        "#torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3gEjf-xMb-N"
      },
      "source": [
        "# Some Notes on our naive model\n",
        "\n",
        "We are going to write a network based on what we have learnt so far.\n",
        "\n",
        "The size of the input image is 28x28x1. We are going to add as many layers as required to reach RF = 32 \"atleast\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\dev\\src\\Learning_2024\\ERA-V2\\.venv\\lib\\site-packages\\torchvision\\datasets\\mnist.py:75: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "tensor(33.3184)\n",
            "tensor(33.3184)\n",
            "tensor(0.1307)\n",
            "tensor(0.3081)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "train_transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_set = datasets.MNIST('../data', train=True, download=True, transform=train_transform)\n",
        "print(train_set.train_data.shape)\n",
        "print(train_set.train_data.float().mean())\n",
        "print(train_dataset.train_data.float().mean())\n",
        "print(train_set.train_data.float().mean()/255)\n",
        "print(train_set.train_data.float().std()/255)\n",
        " # print(train_set.train_data.float().mean()/255)\n",
        "#print(train_dataset.train_data.std(axis=(0,1,2))/255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Input layer:**\n",
        " Input layer has nothing to learn, at it’s core, what it does is just provide the input image’s shape. So no learnable parameters here. Thus number of parameters = 0.\n",
        "#### **CONV layer:** \n",
        "This is where CNN learns, so certainly we’ll have weight matrices. To calculate the learnable parameters here, all we have to do is just multiply the by the shape of width m, height n, previous layer’s filters d and account for all such filters k in the current layer. Don’t forget the bias term for each of the filter. Number of parameters in a CONV layer would be : ((m * n * d)+1)* k), added 1 because of the bias term for each filter. The same expression can be written as follows: ((shape of width of the filter * shape of height of the filter * number of filters in the previous layer+1)*number of filters). Where the term “filter” refer to the number of filters in the current layer.\n",
        "#### **POOL layer:** \n",
        "This has got no learnable parameters because all it does is calculate a specific number, no backprop learning involved! Thus number of parameters = 0.\n",
        "#### **Fully Connected Layer (FC):**\n",
        " This certainly has learnable parameters, matter of fact, in comparison to the other layers, this category of layers has the highest number of parameters, why? because, every neuron is connected to every other neuron! So, how to calculate the number of parameters here? You probably know, it is the product of the number of neurons in the current layer c and the number of neurons on the previous layer p and as always, do not forget the bias term. Thus number of parameters here are: ((current layer neurons c * previous layer neurons p)+1*c)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Sir2LmSVLr_4"
      },
      "outputs": [],
      "source": [
        "class FirstDNN(nn.Module): \n",
        "  def __init__(self):\n",
        "    super(FirstDNN, self).__init__()  ##It is callig the parent class (nn.Module) init function\n",
        "    \n",
        "     ######################### LAYER 1 conv 1 #############################################################################\n",
        "     #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
        "    \n",
        "    #  r_in:1, n_in:28, j_in:1, s:1, r_out:3, n_out:28, j_out:1, k:3 , p =1\n",
        "    #  r_out = r_in + (k  - 1) * j_in = 1 + 2 * 1 = 3\n",
        "    #  n_out = (n_in + 2p - K) / s + 1 = (28 + 2 - 3 ) / 1 + 1 = 28\n",
        "    #  j_out = j_in * s = 1 * 1 = 1\n",
        "     # No of parameters = (k * k *previous input layer + 1 ) * no of filters = (9 + 1) * 32 = 320\n",
        "    # r_in:1, n_in:28, j_in:1, s:1, r_out:3, n_out:28, j_out:1\n",
        "\n",
        "\n",
        "    self.conv1 = nn.Conv2d(1, 32, 3, padding=1)# Edges\n",
        "\n",
        "    ######################### LAYER 2 Conv 2 #############################################################################\n",
        "    #  r_in:3 , n_in:28 , j_in:1 , s:1 , r_out: 5, n_out:28 , j_out:1 , k = 3, p =1\n",
        "    #  r_out = r_in + (k  - 1) * j_in = 3 + 2 *1 = 5\n",
        "    #  n_out = (n_in + 2p - K) / s + 1 = (28 + 2 - 3 ) / 1 + 1 = 28\n",
        "    #  j_out = j_in * s = 1 * 1 = 1\n",
        "    # No of parameters = (k * k *previous input layer + 1 ) * no of filters = (9 *32 + 1) * 64 = 18496\n",
        "    # r_in: 3, n_in:28 , j_in:1 , s:1 , r_out: 5 , n_out:28 , j_out:1\n",
        "\n",
        "\n",
        "\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)#Textures\n",
        "\n",
        "\n",
        "    ######################### LAYER 3 Max #############################################################################\n",
        "    #  r_in: 5, n_in:28 , j_in:1 , s: 2, r_out: 6, n_out:14 , j_out: 2 k = 2 , p =0\n",
        "    #  r_out = r_in + (k  - 1) * j_in = 5 + 1* 1 = 6\n",
        "    #  n_out = (n_in + 2p - K) / s + 1 = (28 - 2 ) / 1 + 1 = 14\n",
        "    #  j_out = j_in * s = 1 * 2 = 2\n",
        "     # No of parameters = 0 (because it is pooling layer)\n",
        "\n",
        "\n",
        "    # r_in:5 , n_in: 28, j_in: 1, s:1 , r_out:6 , n_out: 14, j_out:2\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)#Why parameters is zero\n",
        "\n",
        "\n",
        "\n",
        "    ######################### LAYER 4 Conv3 #############################################################################\n",
        "    # r_in:6 , n_in:14 , j_in:2 , s: 1, r_out: 10, n_out: 14, j_out:2 k = 3, p =1\n",
        "    #  r_out = r_in + (k  - 1) * j_in = 6+ 2*2 = 10\n",
        "    #  n_out = (n_in + 2p - k) / s + 1 = (14 +2 *1 - 3 ) / 1 + 1 = 13 + 1 = 14\n",
        "    #  j_out = j_in * s = 2 * 1 = 2\n",
        "    # No of parameters = (k * k previous input layer + 1 ) * no of filters = (9 *64 + 1) * 128 = 73856\n",
        "\n",
        "   # r_in:6 , n_in: 14, j_in: 2, s:1 , r_out: 10, n_out:14 , j_out:2\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "\n",
        "\n",
        " ######################### LAYER 5 conv 4#############################################################################\n",
        "    # r_in:10 , n_in:14 , j_in:2 , s: 1, r_out:14 , n_out: 14, j_out:2 k = 3, p =1\n",
        "    #  r_out = r_in + (k  - 1) * j_in = 10 +2 * 2 = 14\n",
        "    #  n_out = (n_in + 2p - K) / s + 1 = (14 +2 -3)/ 1 +1 = 14\n",
        "    #  j_out = j_in * s = 2 *1 = 2\n",
        "    # No of parameters = (k * k previous input layer + 1 ) * no of filters = (9 *128 + 1) * 256 = 2,95,168\n",
        "\n",
        "\n",
        "    # # r_in:10 , n_in:14 , j_in:2 , s:1 , r_out:14 , n_out:14 , j_out:2\n",
        "    self.conv4 = nn.Conv2d(128, 256, 3, padding = 1)\n",
        "\n",
        "\n",
        "\n",
        "    ######################### LAYER 6 Max pool #############################################################################\n",
        "    # r_in: 14, n_in: 14, j_in:2 , s:2 , r_out:16, n_out:7 , j_out:4 k = 2 , p =0\n",
        "    #  r_out = r_in + (k  - 1) * j_in = 14 + (1 * 2) = 16\n",
        "    #  n_out = (n_in + 2p - K) / s + 1 = (14 + 0 - 2 )/ 2 + 1 = 7\n",
        "    #  j_out = j_in * s = 2 * 2 = 4\n",
        "    # No of parameters = 0\n",
        "\n",
        "\n",
        "\n",
        "    # # r_in:14 , n_in:14 , j_in:2 , s:2 , r_out: 16, n_out:7 , j_out:4\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "\n",
        "     ######################### LAYER 7 conv 5 #############################################################################\n",
        "    # r_in: 16, n_in:7 , j_in: 4, s: 1, r_out:24 , n_out: 5, j_out:4 k = 3, p = 0\n",
        "    #  r_out = r_in + (k  - 1) * j_in = 16 +2 * 4 = 24\n",
        "    #  n_out = (n_in + 2p - K) / s + 1 = (7 + 0- 3)/1 + 1 = 5\n",
        "    #  j_out = j_in * s = 4*1 = 4\n",
        "    # No of parameters = (k * k previous input layer + 1 ) * no of filters = (9 *256 + 1) * 512 = 11,80,160\n",
        "\n",
        "\n",
        "    self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "     ######################### LAYER 8 conv 6 #############################################################################\n",
        "    # r_in:24 , n_in: 5, j_in:4 , s:1 , r_out:32 , n_out: 3, j_out:4 , k = 3 p =0\n",
        "    #  r_out = r_in + (k  - 1) * j_in = 24 + 2* 4 = 32\n",
        "    #  n_out = (n_in + 2p - K) / s + 1 == (5 + 0- 3)/ 1 + 1 = 3\n",
        "    #  j_out = j_in * s = 4 * 1 = 4\n",
        "    # No of parameters = (k * k previous input layer + 1 ) * no of filters = (9 *512 +1 ) * 1024 ) = 47,19,616\n",
        "\n",
        "    self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "     ######################### LAYER 8 #############################################################################\n",
        "    # r_in: 32, n_in: 3, j_in:4 , s:1 , r_out: 40, n_out:1 , j_out:4, k = 3 ,p =0\n",
        "    #  r_out = r_in + (k  - 1) * j_in = 32 +2 * 4 = 40\n",
        "    #  n_out = (n_in + 2p - K) / s + 1 = (3 + 0- 3 ) / 1 +1 = 1\n",
        "    #  j_out = j_in * s = 4 * 1 = 4\n",
        "    # No of parameters = (k * k previous input layer + 1 ) * no of filters = (9 *1024 +1 ) * 10 ) = 92,170\n",
        "\n",
        "    \n",
        "    self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "# Correct values\n",
        "# https://user-images.githubusercontent.com/498461/238034116-7db4cec0-7738-42df-8b67-afa971428d39.png\n",
        "  def forward(self, x):\n",
        "    x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "    x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "    x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "\n",
        "    x = self.conv7(x)\n",
        "    #x = F.relu(x) # this is the last step. Think what ReLU does to our results at this stage!\n",
        "    x = x.view(-1, 10)\n",
        "    return F.log_softmax(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "sxICO4TTNt2H"
      },
      "outputs": [],
      "source": [
        "model = FirstDNN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt4SyAbIiFp2",
        "outputId": "1f473f1a-293b-469d-afa7-17817974f5e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_25536\\263484479.py:117: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ],
      "source": [
        "summary(model, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "g_vlC-bdNzo1"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0FYVWkGOFBS",
        "outputId": "32763faa-993f-4ee0-8d6d-bccb7188734c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_25536\\263484479.py:117: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=0.06854268163442612 batch_id=468: 100%|██████████| 469/469 [08:07<00:00,  1.04s/it] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0562, Accuracy: 9821/10000 (98%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6agTEkqzz6TZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
